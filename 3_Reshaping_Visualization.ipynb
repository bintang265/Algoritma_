{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coursebook: Reshaping and Visualization**\n",
    "- Part 3 of Data Analytics Specialization\n",
    "- Course Length: 12 hours\n",
    "- Last Updated: May 2021\n",
    "___\n",
    "\n",
    "- Author: [Samuel Chan](https://github.com/onlyphantom)\n",
    "- Developed by [Algoritma](https://algorit.ma)'s product division and instructors team"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "## Top-Down Approach \n",
    "\n",
    "The coursebook is part of the **Data Analytics Specialization** offered by [Algoritma](https://algorit.ma). It takes a more accessible approach compared to Algoritma's core educational products, by getting participants to overcome the \"how\" barrier first, rather than a detailed breakdown of the \"why\". \n",
    "\n",
    "This translates to an overall easier learning curve, one where the reader is prompted to write short snippets of code in frequent intervals, before being offered an explanation on the underlying theoretical frameworks. Instead of mastering the syntactic design of the Python programming language, then moving into data structures, and then the `pandas` library, and then the mathematical details in an imputation algorithm, and its code implementation; we would do the opposite: Implement the imputation, then a succinct explanation of why it works and applicational considerations (what to look out for, what are assumptions it made, when _not_ to use it etc).\n",
    "\n",
    "## Training Objectives\n",
    "\n",
    "This coursebook is intended for participants who have completed the preceding courses offered in the **Data Analytics Developer Specialization**. This is the third course, **Reshaping and Visualization**.\n",
    "\n",
    "The coursebook focuses on:\n",
    "- Stacking and Unstacking\n",
    "- Working with MultiIndex DataFrames\n",
    "- Reshaping your DataFrame with Melt\n",
    "- Using Group By Effectively\n",
    "- Visual Data Exploratory\n",
    "\n",
    "At the end of this course is a Learn by Building section, where you are expected to apply all that you've learned on a new dataset, and attempt the given questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducible Environment\n",
    "\n",
    "There are some new packages we'll use in this material. Usually, we can use `pip install`/`conda install` to install new libraries to our environment. But for now, let's try on another approach on preparing libraries needed for a certain project.\n",
    "\n",
    "Imagine you're working with your team on a collaborative project. You initialize the project with certain dependencies and versions on your computer and all goes well. Later on, you need to 'ship' that project to your team which requires them to set up the same environment as yours. What would you do then to make sure that program will also runs smoothly on their machine? \n",
    "\n",
    "This is where you need to make your environment reproducible by creating a `requirements.txt` file.\n",
    "\n",
    "If you browse on `/assets` directory on this repository, you'll find a file called `requirements.txt`. This file is used for specifying what python packages are required to run a certain project. If you open up the file, you will see something that looks similar to this:\n",
    "\n",
    "backcall==0.1.0  \n",
    "certifi==2019.11.28  \n",
    "chardet==3.0.4  \n",
    "cycler==0.10.0  \n",
    "decorator==4.4.0  \n",
    "idna==2.9  \n",
    "ipython==7.7.0  \n",
    "......\n",
    "\n",
    "\n",
    "Notice we have a line for each package, then a version number. This is important because as you start developing your python applications, you will develop the application with specific versions of the packages in mind. In simple, `requirements.txt` helps to keep track of what version of each package you are using to prevent unexpected changes.\n",
    "\n",
    "## Importing Requirements\n",
    "\n",
    "We have discussed what the requirement files is for but how do we use it? Since we don't want to manually install and track every package needed for a certain project, let's try to import the requirements with the following steps:\n",
    "\n",
    "**Step 1**: Prepare a \"blank\" new environment and activate it\n",
    "\n",
    "```\n",
    "conda env create -n <ENV_NAME> python=<PYTHON_VERSION>\n",
    "conda activate <ENV_NAME>\n",
    "```\n",
    "\n",
    "**Step 2**: Navigate to the folder with your `requirements.txt`\n",
    "\n",
    "```\n",
    "cd <PATH_TO_REQUIREMENTS>\n",
    "```\n",
    "\n",
    "**Step 3**: Install the requirements\n",
    "\n",
    "```\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "You have now successfuly installed all the requirements needed on this material! For your convenience, don't forget to link your new environment to jupyter-notebook using kernel:\n",
    "\n",
    "```\n",
    "pip install ipykernel\n",
    "python -m ipykernel install --user --name=<ENV_NAME>\n",
    "```\n",
    "\n",
    "## Exporting Requirements\n",
    "\n",
    "The `pip install` command always installs the latest published version of a package, but sometimes, you may want to install a specific version that you know works on your project.\n",
    "\n",
    "Requirement files allow you to specify exactly which packages and versions should be installed. You can follow these steps to generate your requirement files:\n",
    "\n",
    "**Step 1**: Activate desired environment\n",
    "\n",
    "```\n",
    "conda activate <ENV_NAME>\n",
    "```\n",
    "\n",
    "**Step 2**: Navigate to the folder where you want to save the `requirements.txt`\n",
    "```\n",
    "cd <PATH_TO_REQUIREMENTS_FOLDER>\n",
    "```\n",
    "\n",
    "**Step 3**: Freeze the environment\n",
    "\n",
    "```\n",
    "pip freeze > requirements.txt\n",
    "```\n",
    "The `freeze` command dumps all the packages and their versions to a standardized output. You can save it by any name you want but the convention is to name it as requirements.txt.\n",
    "\n",
    "Now that you've discovered how to make your environment reproducible, we can back to our main focus of this week material; data reshaping and visualisation with pandas!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling and Reshaping\n",
    "\n",
    "In the previous two courses, we've got our hands on a few common techniques and learned how to explore data using `pandas` built-in methods. Specifically, we've  in the first and second part of this series how to use the following inspection, diagnostic and exploratory tools: \n",
    "\n",
    "**Data Inspection**\n",
    "- `.head()` and `.tail()`\n",
    "- `.describe()`\n",
    "- `.shape` and `.size`\n",
    "- `.axes`\n",
    "- `.dtypes`\n",
    "- Subsetting using `.loc`, `.iloc` and conditionals\n",
    "---\n",
    "**Diagnostic and Exploratory**\n",
    "- Tables\n",
    "- Cross-Tables and Aggregates\n",
    "- Using `aggfunc` for aggregate functions\n",
    "- Pivot Tables\n",
    "- Working with DateTime\n",
    "- Working with Categorical Data\n",
    "- Duplicates and Missing Value Treatment\n",
    "\n",
    "The first half of this course serves as an extension from the last. We'll pick up some new techniques to supplement our EDA toolset. Let us begin with reshaping techniques. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_datareader import data\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol = ['AAPL', 'FB', 'GOOGL']\n",
    "source = 'yahoo'\n",
    "start_date = '2018-01-01'\n",
    "end_date = '2022-01-01'\n",
    "stock = data.DataReader(symbol, source, start_date, end_date)\n",
    "stock.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do not have the `pandas_datareader` module installed, or if you're following along this coursebook without an active connection, you can instead load it from the serialized object I stored in your `data_cache` folder. \n",
    "\n",
    "Creating the DataFrame object by reading from `pickle`:\n",
    "- `stock = pd.read_pickle('data_cache/stock')`\n",
    "\n",
    "Serializing the DataFrame object to a byte stream using `pickle`:\n",
    "- `stock.to_pickle('data_cache/stock')`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "import pickle\n",
    "a = 12\n",
    "f= open(\"haha\",\"wb\")\n",
    "pickle.dump(a, f)\n",
    "f.close()\n",
    "\n",
    "f=open(\"haha\", \"rb\")\n",
    "b = pickle.load(f)\n",
    "f.close()\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stock.to_pickle('data_cache/stock') # write dataframe into pickle \n",
    "stock = pd.read_pickle('data_cache/stock')\n",
    "stock.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the data frame is a multi-index data frame. If you pay close attention, you can see a 2 levels of column axis: `Attributes` and `Symbols`. If you were to subset the data using square bracket, you will be accessing the highest level index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access attribute `High`\n",
    "stock['Close']\n",
    "\n",
    "# Otherwise, this code will raise an error \n",
    "# stock['AAPL']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subsetting the `Close` column from the data frame will leave us with a single index column from the `Symbols` level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dive Deeper:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a DataFrame by subsetting only the `Close` columns. Name it `closingprice`. Then, use `.isna().sum()` to count the number of missing values in each of the columns present in `closingprice`.\n",
    "\n",
    "If there are any missing values, use the `.fillna(method='ffill')` method to fill those missing values:\n",
    "\n",
    "<!--\n",
    "closingprice = stock['Close']\n",
    "closingprice.isna().sum()\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your solution code here \n",
    "closingprice = ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you pay close attention to the index of `stock`, you may already realized by now that there are days where no records were present. 2018-01-01, 2018-01-06, and 2018-01-07 were absent from our DataFrame because they happen to fall on weekends.\n",
    "\n",
    "While the trading hours of [different stock markets differ](https://www.maybank-ke.com.sg/markets/markets-listing/trading-hours/) (the NYSE for example open its market floor from 9.30am to 4pm five days a week), on weekends as well as federal holidays all stock exchanges are closed for business.\n",
    "\n",
    "We can create (or recreate) the index by passing in our own values. In the following cell we created a date range and create the index using that new date range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.date_range(start=\"2019-01-01\", end=\"2019-03-31\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closingprice = stock['Close']\n",
    "quarter1 = pd.date_range(start=\"2019-01-01\", end=\"2019-03-31\")\n",
    "closingprice = closingprice.reindex(quarter1)\n",
    "closingprice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use forward-fill to fill the `NA` values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your solution code here \n",
    "closingprice.ffill()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `stack()` and `unstack()`\n",
    "\n",
    "`stack()` stack the prescribed level(s) from columns to index and is particularly useful on DataFrames having a multi-level columns. It does so by \"shifting\" the columns to create new levels on its index. \n",
    "\n",
    "This is easier understood when we just see an example. Notice that `stock` has a 2-level column (Attributes and Symbols) and 1-level index (Date):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stock.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we stack the `stock` DataFrame, we shrink the number of levels on its column by one: `stock` now has 1-level column named `Attributes`: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`unstack()` does the opposite: it \"shifts\" the levels from index axis onto column axis. **Try and create a stack DataFrame, and then apply `unstack` on the new DataFrame to see it return to the original shape:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your code to try out .unstack() method here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dive Deeper**\n",
    "\n",
    "Answer these following questions to ensure that you can continue for the next session:\n",
    "1. How to swap the position (level) of Symbols and Attributes ? \n",
    "2. Based on your knowledge, what company (symbol) worth invest on ? (You may look on its fluctuations, means, etc)\n",
    "\n",
    "<!--\n",
    "# answer 1\n",
    "stock.stack(level=0).unstack(level=1)\n",
    "\n",
    "# answer 2\n",
    "# Overal Growth Values\n",
    "stock['Close'].iloc[-1,:] - stock['Close'].iloc[0,:]\n",
    "\n",
    "# Oveal Growth Percentage\n",
    "(stock['Close'].iloc[-1,:] - stock['Close'].iloc[0,:]) / stock['Close'].iloc[0,:]\n",
    "\n",
    "# Standard Deviation \n",
    "stock['Close'].std() / stock['Close'].mean()\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your solution code here \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Knowledge Check:** Stack and Unstack\n",
    "\n",
    "Which of the following statement is correct?\n",
    "\n",
    "- [ ] `stack()` changes the DataFrame from wide to long\n",
    "- [ ] `unstack()` changes the DataFrame from long to wide\n",
    "- [ ] `unstack()` changes the DataFrame from wide to long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Melt\n",
    "\n",
    "Speaking of reshaping a DataFrame from wide format to long, another method that should be in your toolset is `melt()`. Consider the following DataFrame, which is created from `pandas` MultiIndex Slicers method, `.xs()` (Abbreviation for 'Cross Section'):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl = stock.xs(key = 'GOOGL', level='Symbols', axis=1)\n",
    "aapl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DataFrame above is wide: it has 329 rows and 6 columns. The `melt()` function gathers all the columns into one and store the value corresponding to each column such that the resulting DataFrame has 329 * 6 = 1,974 rows, along with the identifier and values columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl_melted = aapl.melt()\n",
    "aapl_melted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl_melted.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Knowledge Check :**\n",
    "What's the difference betweent melt and stack ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can optionally specify one or more columns to be identifier variables (`id_vars`), which treat all other columns as value variables (`value_vars`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl.reset_index().melt(id_vars=['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl.reset_index().melt(value_vars=['High', 'Low'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Knowledge Check:** Missing Values\n",
    "\n",
    "Given a data below, fill the missing values in `aapl` using appropriate method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "march = pd.date_range(start=\"2018-03-01\", end=\"2019-03-31\")\n",
    "aapl = stock.xs('AAPL', level='Symbols', axis=1)\n",
    "aapl = aapl.reindex(march)\n",
    "aapl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your code to fill the missing values in `aapl`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas and Matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surely this is the point where a data analyst whip up some flashy charts using the popular `matplotlib` library? \n",
    "\n",
    "Well - yes. Even better, we're going to use the `DataFrame.plot()` method, built-into `pandas` which in turn calls `matplotlib` plotting functions under-the-hood. Notice that we added `matplotlib.pyplot` as an import, even though our code will not explicitly call `matplotlib` but rely on `pandas` implementation.\n",
    "\n",
    "Now let's take a look at apple stock data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aapl.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best way to demonstrate the efficiency gains of `DataFrame.plot()` is to see it in action. We will call `.plot()` directly on our `DataFrame` - `pandas` take care of the  matplotlib code that, [by matplotlib's own admission](https://matplotlib.org/tutorials/introductory/usage.html#sphx-glr-tutorials-introductory-usage-py), _can be daunting to many new users_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock['Close'].head(10).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can [customize our plots with style sheets](https://matplotlib.org/users/style_sheets.html) but a handy reference is within reach. You can substitute 'default' for any one of the styles available and re-run the plotting code to see the styles being applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(plt.style.available)\n",
    "plt.style.use('default')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the `.plot()` method is called on a DataFrame object, we can have an indexed DataFrame with multiple columns and `plot` will handle these using its default options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl.loc[:, ['High', 'Low', 'Adj Close']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'mass': [0.330, 4.87 , 5.97],\n",
    "              'radius': [2439.7, 6051.8, 6378.1]},\n",
    "             index=['Mercury', 'Venus', 'Earth'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Visualization\n",
    "one column visualization:\n",
    "- `.plot.bar()` or `.plot.barh()` for bar plots\n",
    "- `.plot.hist()` for histogram \n",
    "- `.plot.box()` or `.boxplot()` for boxplot\n",
    "- `.plot.kde()` or `.plot.density()` for density plots\n",
    "- `.plot.area()` for area plots \n",
    "- `.plot.pie()` for pie plots\n",
    "\n",
    "two column visualization:\n",
    "- `.plot.scatter()` for scatter plots\n",
    "- `.plot.hexbin()` for hexagonal bin plots\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group By\n",
    "\n",
    "Reshaping data is an important component of any data wrangling toolkit as it allows the analyst to \"massage\" the data into the desired shape for further processing. \n",
    "\n",
    "Another equally important technique is the group by operation. Analysts having some experience with SQL or other data analysis toolsets (R's `tidyverse` for example) will find the group by operation a familiar strategy in many analysis-heavy workflow.\n",
    "\n",
    "Consider the following DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_adj = stock.stack()\n",
    "stock_adj['Volume USD'] = stock_adj['Volume'] * stock_adj['Adj Close']\n",
    "stock_adj = stock_adj.unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume = stock_adj.xs('Volume USD', level='Attributes', axis=1)\n",
    "volume = volume.round(2)\n",
    "volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the data frame shows amount of daily volume transaction, say we would like to compare the average daily transaction for AAPL, FB, and GOOGL. Let's perform a melting function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_melted = volume.melt()\n",
    "volume_melted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supposed we would like to compare the average volume transaction between each stock price. On average, which of the 3 stocks has the highest average daily transaction volume?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_melted.groupby(['Symbols']).mean().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Barchart for Comparison\n",
    "    \n",
    "Say we would like to compare the average daily volume sold from the companies. To do that, we will need to extract volume attribute from our dataframe, and perform a melt function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_melted.groupby('Symbols').mean().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to compare the visualization to the numerical figure, it is far way easier to compare each stock's average volume. Now let's consider this following data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl = stock.xs('AAPL', level='Symbols', axis=1)\n",
    "aapl = aapl.round(2)\n",
    "aapl['Close_Diff'] = aapl['Close'].diff()\n",
    "aapl['Weekday'] = aapl.index.day_name()\n",
    "aapl['Month'] = aapl.index.month_name()\n",
    "aapl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pay special attention to how the Close_Diff column was created. It's the difference between the Close value of a stock price on a given day and the following day.\n",
    "\n",
    "Supposed we want to compare the Close_Diff between each Weekday; On average, does Tuesday record a higher difference between the Close price of Apple stock compared to a Thursday?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl.groupby('Weekday').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to create the same bar chart using `plot` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl.groupby('Weekday').mean()['Close_Diff'].plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also improve our visualization efficiency by average transaction volume values in advance, so then the bars from our plot will be arranged based on the value, rather than the weekday's alphabetical order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aapl.groupby('Weekday').mean()['Close_Diff'].plot.bar()\n",
    "aapl.groupby('Weekday').mean()['Close_Diff'].sort_values(ascending=False).plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also created a manually ordered index by specifying the order of the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wday = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\"]\n",
    "\n",
    "aapl_wday = aapl.groupby('Weekday').mean()['Close_Diff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl_wday.index = pd.CategoricalIndex(aapl_wday.index,\\\n",
    "                                      categories=wday,\\\n",
    "                                      ordered=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl_wday.sort_index().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Grouped Barchart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `closingprice`,  we can try to visualize using a grouped barchart to compare each month's closing price for the first quarter of 2019 and compare it for the 3 stocks.\n",
    "\n",
    "First, take a look at `closingprice` and make sure that the data has no missing values. If it has, fill it using appropriate method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closingprice.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your solution code here \n",
    "\n",
    "# Fill misssing value if any\n",
    "closingprice = closingprice.ffill().bfill()\n",
    "\n",
    "# Create new column called 'Month', denoting the month name of the date\n",
    "closingprice['Month'] = closingprice.index.month_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closingprice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have the `Month` columns, let's group it by Month and see the resulting DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_closing = closingprice.groupby('Month').mean()\n",
    "average_closing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_closing.sort_index().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if you want to reorder the month, we have to set the index as an ordered categorical values (See Exploratory Data Analysis materials if you need to recall). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months= ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_closing.index = pd.CategoricalIndex(average_closing.index,\\\n",
    "                                            categories=months,\\\n",
    "                                            ordered=True)\n",
    "\n",
    "average_closing.sort_index().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A full reference to [the official documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.html) on this method would be outside the scope of this coursebook, but is worth a read. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining `agg` and `groupby`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have explored several pandas aggregational toolkit, such as:\n",
    "- `pd.crosstab()`\n",
    "- `pd.pivot_table()`\n",
    "\n",
    "In this chapter, we'll explore another pandas' aggregating tools:\n",
    "- `groupby` aggregation.\n",
    "\n",
    "**Disucission:**\n",
    "\n",
    "(`pivot_table` & `pd.crosstab` equivalency)\n",
    "\n",
    "The `pivot_table` method and the `crosstab` function can both produce the exact same results with the same shape. They both share the parameters; `index`, `columns`, `values`, and `aggfunc`. \n",
    "\n",
    "The major difference on the surface is that `crosstab` is a function and not a DataFrame method. This forces you to use columns as Series and not string names for the parameters.\n",
    "\n",
    "1. Suppose you want to compare the number of total transactions over Weekdays of each quarter period. Create a `pivot_table` that solve the problem!\n",
    "\n",
    "\n",
    "2. Try to reproduce the same result by using `crosstab`\n",
    "\n",
    "\n",
    "3. What if, instead of compare the total transactions, you want to compare the total revenue from the same period? Use both `pivot_table` and `crosstab` as the solution. Discuss with your friend, which method is more relevant in this case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pay attention to the following group by operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock.stack().reset_index().groupby('Symbols').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock.stack().reset_index().groupby('Symbols').agg({\n",
    "    'Close': 'mean',\n",
    "    'High': 'max',\n",
    "    'Low': 'min'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say we would like to know a glimpse of the maximum stock price, minimum stock price, and the average of closing price from the 3 companies. To do that, we'll need to combine `groupby` with `agg` and map each column with its designated of the aggregation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock.stack().reset_index().groupby('Symbols').agg({\n",
    "    'Close': 'median',\n",
    "    'High': 'max',\n",
    "    'Low': 'min'\n",
    "}).plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Knowledge Check:** Using `plot`\n",
    "\n",
    "Consider the following data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "stock['YearMonth'] = pd.to_datetime(stock.index.date).to_period('M')\n",
    "monthly_closing = stock.groupby('YearMonth').mean().loc[:,['Close','Low', 'High']]\n",
    "monthly_closing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which of the following will be appropriate plot to use?\n",
    "\n",
    "- [ ] Line plot -> .plot()\n",
    "- [ ] Scatter plot -> .plot.scatter(x=? , y=?)\n",
    "- [ ] Bar plot -> .plot.bar()\n",
    "- [ ] Box plot -> .plot.box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code below\n",
    "\n",
    "## -- Solution code\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
